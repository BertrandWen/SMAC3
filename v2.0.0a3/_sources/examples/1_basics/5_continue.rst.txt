
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/1_basics/5_continue.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_examples_1_basics_5_continue.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_1_basics_5_continue.py:


Continue an Optimization
^^^^^^^^^^^^^^^^^^^^^^^^

SMAC offers the possibility to continue an optimization. In this example, an optimization of a simple quadratic function
is continued. We use a custom callback, to artificially stop the first optimization.

.. GENERATED FROM PYTHON SOURCE LINES 8-92




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [INFO][abstract_initial_design.py:81] Reducing the number of initial configurations from 10 to 5 (max_ratio == 0.1).
    [INFO][abstract_initial_design.py:133] Using 5 initial design configurations and 0 additional configurations.
    [INFO][abstract_intensifier.py:288] Using only one seed for deterministic scenario.
    [INFO][abstract_intensifier.py:479] Added config 3ce33d as new incumbent because there are no incumbents yet.
    [INFO][abstract_intensifier.py:551] Added config c53710 and rejected config 3ce33d as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- x: -2.331214789301157 -> 2.1608731895685196
    [INFO][abstract_intensifier.py:551] Added config 378ada and rejected config c53710 as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- x: 2.1608731895685196 -> 2.151893663724195
    [INFO][smbo.py:219] A callback returned False. Abort is requested.
    [INFO][smbo.py:308] Shutting down because the stop flag was set.
    [INFO][abstract_initial_design.py:81] Reducing the number of initial configurations from 10 to 5 (max_ratio == 0.1).
    [INFO][abstract_initial_design.py:133] Using 5 initial design configurations and 0 additional configurations.
    [INFO][smbo.py:458] Continuing from previous run.
    [INFO][abstract_intensifier.py:270] Added existing seed 209652396 from runhistory to the intensifier.
    [INFO][abstract_intensifier.py:288] Using only one seed for deterministic scenario.
    [INFO][abstract_intensifier.py:551] Added config b24989 and rejected config 378ada as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- x: 2.151893663724195 -> 2.1312568530072307
    [INFO][abstract_intensifier.py:551] Added config 3f8dcf and rejected config b24989 as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- x: 2.1312568530072307 -> 2.1002821620968657
    [INFO][abstract_intensifier.py:551] Added config 476875 and rejected config 3f8dcf as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- x: 2.1002821620968657 -> 2.0797518811302504
    [INFO][abstract_intensifier.py:551] Added config 42faf0 and rejected config 476875 as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- x: 2.0797518811302504 -> 1.650576079787803
    [INFO][abstract_intensifier.py:551] Added config 1bf166 and rejected config 42faf0 as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- x: 1.650576079787803 -> 1.282673465040193
    [INFO][abstract_intensifier.py:551] Added config 66c687 and rejected config 1bf166 as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- x: 1.282673465040193 -> 1.2769786999051718
    [INFO][abstract_intensifier.py:551] Added config 0abffe and rejected config 66c687 as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- x: 1.2769786999051718 -> -0.38520637747068154
    [INFO][abstract_intensifier.py:551] Added config 5c8ec1 and rejected config 0abffe as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- x: -0.38520637747068154 -> -0.22214750836837016
    [INFO][abstract_intensifier.py:551] Added config 7b0e4b and rejected config 5c8ec1 as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- x: -0.22214750836837016 -> -0.21826561478479078
    [INFO][abstract_intensifier.py:551] Added config 249efc and rejected config 7b0e4b as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- x: -0.21826561478479078 -> -0.20897256967682676
    [INFO][abstract_intensifier.py:551] Added config 3f4c58 and rejected config 249efc as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- x: -0.20897256967682676 -> -0.20289686897024417
    [INFO][abstract_intensifier.py:551] Added config fce6eb and rejected config 3f4c58 as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- x: -0.20289686897024417 -> 0.13034334833880834
    [INFO][abstract_intensifier.py:551] Added config 389e22 and rejected config fce6eb as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- x: 0.13034334833880834 -> -0.05219773454305354
    [INFO][abstract_intensifier.py:551] Added config e511a1 and rejected config 389e22 as incumbent because it is not better than the incumbents on 1 instances:
    [INFO][configspace.py:175] --- x: -0.05219773454305354 -> 0.02098593273791316
    [INFO][smbo.py:295] Finished 50 trials.
    [INFO][smbo.py:303] Configuration budget is exhausted:
    [INFO][smbo.py:304] --- Remaining wallclock time: inf
    [INFO][smbo.py:305] --- Remaining cpu time: inf
    [INFO][smbo.py:306] --- Remaining trials: 0
    [INFO][abstract_intensifier.py:288] Using only one seed for deterministic scenario.
    Default cost: 25.0
    Incumbent cost of first run: 4.630646339976339
    [INFO][abstract_intensifier.py:288] Using only one seed for deterministic scenario.
    Incumbent cost of continued run: 0.0004404093728802153






|

.. code-block:: default


    from __future__ import annotations
    from ConfigSpace import Configuration, ConfigurationSpace, Float


    from smac import HyperparameterOptimizationFacade as HPOFacade
    from smac import Scenario, Callback
    from smac.runhistory import TrialInfo, TrialValue
    from smac.main.smbo import SMBO

    __copyright__ = "Copyright 2021, AutoML.org Freiburg-Hannover"
    __license__ = "3-clause BSD"


    class StopCallback(Callback):
        def __init__(self, stop_after: int):
            self._stop_after = stop_after

        def on_tell_end(self, smbo: SMBO, info: TrialInfo, value: TrialValue) -> bool | None:
            """Called after the stats are updated and the trial is added to the runhistory. Optionally, returns false
            to gracefully stop the optimization.
            """
            if smbo.runhistory.finished == self._stop_after:
                return False

            return None


    class QuadraticFunction:
        @property
        def configspace(self) -> ConfigurationSpace:
            cs = ConfigurationSpace(seed=0)
            x = Float("x", (-5, 5), default=-5)
            cs.add_hyperparameters([x])

            return cs

        def train(self, config: Configuration, seed: int = 0) -> float:
            """Returns the y value of a quadratic function with a minimum at x=0."""
            x = config["x"]
            return x * x


    if __name__ == "__main__":
        model = QuadraticFunction()

        # Scenario object specifying the optimization "environment"
        scenario = Scenario(model.configspace, deterministic=True, n_trials=50)
        stop_after = 10

        # Now we use SMAC to find the best hyperparameters
        smac = HPOFacade(
            scenario,
            model.train,  # We pass the target function here
            callbacks=[StopCallback(stop_after=stop_after)],
            overwrite=True,  # Overrides any previous results that are found that are inconsistent with the meta-data
        )

        incumbent = smac.optimize()
        assert smac.runhistory.finished == stop_after

        # Now, we want to continue the optimization
        # Make sure, we don't overwrite the last run
        smac2 = HPOFacade(
            scenario,
            model.train,
            overwrite=False,
        )

        # Check whether we get the same incumbent
        assert smac.intensifier.get_incumbent() == smac2.intensifier.get_incumbent()
        assert smac2.runhistory.finished == stop_after

        # And now we finish the optimization
        incumbent2 = smac2.optimize()

        default_cost = smac.validate(model.configspace.get_default_configuration())
        print(f"Default cost: {default_cost}")

        incumbent_cost = smac.validate(incumbent)
        print(f"Incumbent cost of first run: {incumbent_cost}")

        incumbent_cost = smac2.validate(incumbent2)
        print(f"Incumbent cost of continued run: {incumbent_cost}")


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.985 seconds)


.. _sphx_glr_download_examples_1_basics_5_continue.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 5_continue.py <5_continue.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 5_continue.ipynb <5_continue.ipynb>`

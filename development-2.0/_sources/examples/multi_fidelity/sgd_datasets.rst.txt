
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/multi_fidelity/sgd_datasets.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_examples_multi_fidelity_sgd_datasets.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_multi_fidelity_sgd_datasets.py:


Stochastic Gradient Descent On Multiple Datasets
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Example for optimizing a Multi-Layer Perceptron (MLP) across multiple (dataset) instances.

Alternative to budgets, here we consider instances as a fidelity type. An instance represents a specific
scenario/condition (e.g. different datasets, subsets, transformations) for the algorithm to run. SMAC then returns the
algorithm that had the best performance across all the instances. In this case, an instance is a binary dataset i.e.,
digit-2 vs digit-3.

If we use instance as our fidelity, we need to initialize scenario with argument instance. In this case the argument
budget is no longer required by the target function.

.. GENERATED FROM PYTHON SOURCE LINES 15-152




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Default cost: 0.14
    [INFO][initial_design.py:85] Ignoring `configs` and `n_configs` since `n_configs_per_hyperparameter` is given.
    [INFO][base_smbo.py:144] Running initial design...
    [INFO][initial_design.py:155] Retrieving 40 configurations for the initial design.
    [WARNING][parallel_scheduling.py:150] Hyperband is executed with 1 worker(s) only. However, your system supports up to 2 workers. Consider increasing the workers in the scenario.
    [WARNING][successive_halving.py:365] The target algorithm is specified to be non-deterministic, but number of seeds to evaluate are set to 1. Consider setting `n_seeds` > 1.
    [INFO][successive_halving.py:497] Successive Halving configuration: budget type = INSTANCES, Initial budget = 1.00, Max. budget = 45.00, eta = 3.00
    [INFO][hyperband.py:437] Finished Hyperband iteration-step 1-1 with initial budget 1.
    [INFO][successive_halving.py:497] Successive Halving configuration: budget type = INSTANCES, Initial budget = 1.00, Max. budget = 45.00, eta = 3.00
    [INFO][successive_halving.py:809] First run and no incumbent provided. Challenger is assumed to be the incumbent.
    [INFO][abstract_intensifier.py:345] Challenger (0.0028) is better than incumbent (0.0111) on 1 runs.
    [INFO][abstract_intensifier.py:365] Changes in incumbent:
    [INFO][abstract_intensifier.py:368] --- alpha: 0.5488135039273248 -> 0.6027633760716439
    [INFO][abstract_intensifier.py:368] --- eta0: 0.0006273927602293597 -> 0.03077201812975574
    [INFO][abstract_intensifier.py:368] --- l1_ratio: 0.317983179393976 -> 0.06414749634878436
    [INFO][abstract_intensifier.py:345] Challenger (0.0) is better than incumbent (0.0028) on 1 runs.
    [INFO][abstract_intensifier.py:365] Changes in incumbent:
    [INFO][abstract_intensifier.py:368] --- alpha: 0.6027633760716439 -> 0.11827442586893322
    [INFO][abstract_intensifier.py:368] --- eta0: 0.03077201812975574 -> 9.614170248896958e-05
    [INFO][abstract_intensifier.py:368] --- l1_ratio: 0.06414749634878436 -> 0.24875314351995803
    [INFO][successive_halving.py:641] Finished Successive Halving iteration-step 1-1 with budget [1.67 / 45] and 27 evaluated challenger(s).
    [INFO][successive_halving.py:641] Finished Successive Halving iteration-step 1-2 with budget [5.00 / 45] and 9 evaluated challenger(s).
    [INFO][successive_halving.py:641] Finished Successive Halving iteration-step 1-3 with budget [15.00 / 45] and 3 evaluated challenger(s).
    [INFO][successive_halving.py:641] Finished Successive Halving iteration-step 1-4 with budget [45.00 / 45] and 1 evaluated challenger(s).
    [INFO][hyperband.py:437] Finished Hyperband iteration-step 1-2 with initial budget 5.
    [INFO][successive_halving.py:497] Successive Halving configuration: budget type = INSTANCES, Initial budget = 1.00, Max. budget = 45.00, eta = 3.00
    [INFO][successive_halving.py:641] Finished Successive Halving iteration-step 1-1 with budget [5.00 / 45] and 9 evaluated challenger(s).
    [INFO][successive_halving.py:641] Finished Successive Halving iteration-step 1-2 with budget [15.00 / 45] and 3 evaluated challenger(s).
    [INFO][successive_halving.py:641] Finished Successive Halving iteration-step 1-3 with budget [45.00 / 45] and 1 evaluated challenger(s).
    [INFO][hyperband.py:437] Finished Hyperband iteration-step 1-3 with initial budget 15.
    [INFO][successive_halving.py:497] Successive Halving configuration: budget type = INSTANCES, Initial budget = 1.00, Max. budget = 45.00, eta = 3.00
    [INFO][successive_halving.py:641] Finished Successive Halving iteration-step 1-1 with budget [15.00 / 45] and 6 evaluated challenger(s).
    [INFO][successive_halving.py:641] Finished Successive Halving iteration-step 1-2 with budget [45.00 / 45] and 2 evaluated challenger(s).
    [INFO][hyperband.py:437] Finished Hyperband iteration-step 1-4 with initial budget 45.
    [INFO][successive_halving.py:497] Successive Halving configuration: budget type = INSTANCES, Initial budget = 1.00, Max. budget = 45.00, eta = 3.00
    [INFO][facade.py:324] Final Incumbent: {'alpha': 0.11827442586893322, 'eta0': 9.614170248896958e-05, 'l1_ratio': 0.24875314351995803, 'learning_rate': 'adaptive'}
    [INFO][facade.py:325] Estimated cost: 0.0082313892690867
    Incumbent cost: 0.01






|

.. code-block:: default


    from __future__ import annotations
    import itertools
    import warnings

    import numpy as np
    from ConfigSpace import ConfigurationSpace, Float, Categorical, Configuration
    from sklearn import datasets
    from sklearn.linear_model import SGDClassifier
    from sklearn.model_selection import StratifiedKFold, cross_val_score

    from smac.configspace import ConfigurationSpace
    from smac import MultiFidelityFacade, Scenario

    __copyright__ = "Copyright 2021, AutoML.org Freiburg-Hannover"
    __license__ = "3-clause BSD"


    class DigitsDataset:
        def __init__(self):
            self._data = datasets.load_digits()

        def get_instances(self) -> list[str]:
            """Create instances from the dataset which include two classes only."""
            return [f"{classA}-{classB}" for classA, classB in itertools.combinations(self._data.target_names, 2)]

        def get_instance_features(self) -> dict[str, list[int | float]]:
            """Returns the mean and variance of all instances as features."""
            features = {}
            for instance in self.get_instances():
                data, _ = self.get_instance_data(instance)
                features[instance] = [np.mean(data), np.var(data)]

            return features

        def get_instance_data(self, instance: str) -> tuple[np.ndarray, np.ndarray]:
            """Retrieve data from the passed instance."""
            # We split the dataset into two classes
            classA, classB = instance.split("-")
            indices = np.where(np.logical_or(int(classA) == self._data.target, int(classB) == self._data.target))

            data = self._data.data[indices]
            target = self._data.target[indices]

            return data, target


    class SGD:
        def __init__(self, dataset) -> None:
            self.dataset = dataset

        @property
        def configspace(self) -> ConfigurationSpace:
            """Build the configuration space which defines all parameters and their ranges for the SGD classifier."""
            cs = ConfigurationSpace()

            # We define a few possible parameters for the SGD classifier
            alpha = Float("alpha", (0, 1), default=1.0)
            l1_ratio = Float("l1_ratio", (0, 1), default=0.5)
            learning_rate = Categorical("learning_rate", ["constant", "invscaling", "adaptive"], default="constant")
            eta0 = Float("eta0", (0.00001, 1), default=0.1, log=True)
            # Add the parameters to configuration space
            cs.add_hyperparameters([alpha, l1_ratio, learning_rate, eta0])

            return cs

        def train(self, config: Configuration, instance: str, seed: int = 0):
            """Creates a SGD classifier based on a configuration and evaluates it on the
            digits dataset using cross-validation."""

            with warnings.catch_warnings():
                warnings.filterwarnings("ignore")

                # SGD classifier using given configuration
                clf = SGDClassifier(
                    loss="log",
                    penalty="elasticnet",
                    alpha=config["alpha"],
                    l1_ratio=config["l1_ratio"],
                    learning_rate=config["learning_rate"],
                    eta0=config["eta0"],
                    max_iter=30,
                    early_stopping=True,
                    random_state=seed,
                )

                # get instance
                data, target = self.dataset.get_instance_data(instance)

                cv = StratifiedKFold(n_splits=4, random_state=seed, shuffle=True)  # to make CV splits consistent
                scores = cross_val_score(clf, data, target, cv=cv)

            return 1 - np.mean(scores)


    if __name__ == "__main__":
        dataset = DigitsDataset()
        model = SGD(dataset)

        scenario = Scenario(
            model.configspace,
            walltime_limit=40,  # We want to optimize for 40 seconds
            n_trials=5000,  # We want to try max 5000 different configurations
            min_budget=1,  # Use min one instance
            max_budget=45,  # Use max 45 instances (if we have a lot of instances we could constraint it)
            instances=dataset.get_instances(),
            instance_features=dataset.get_instance_features(),
            instance_order="shuffle_once",  # Shuffle instances beforehand
        )

        # Calculate the mean cost of all instances for the default configuration
        default_costs = []
        for instance in dataset.get_instances():
            cost = model.train(
                config=model.configspace.get_default_configuration(),
                instance=instance,
            )
            default_costs += [cost]
        print(f"Default cost: {round(np.mean(default_costs), 2)}")

        # Create our SMAC object and pass the scenario and the train method
        smac = MultiFidelityFacade(
            scenario,
            model.train,
            overwrite=True,
        )
        incumbent = smac.optimize()

        # Calculate the mean cost of all instances for the incumbent
        incumbent_costs = []
        for instance in dataset.get_instances():
            cost = model.train(
                config=incumbent,
                instance=instance,
            )
            incumbent_costs += [cost]
        print(f"Incumbent cost: {round(np.mean(incumbent_costs), 2)}")


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  44.780 seconds)


.. _sphx_glr_download_examples_multi_fidelity_sgd_datasets.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: sgd_datasets.py <sgd_datasets.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: sgd_datasets.ipynb <sgd_datasets.ipynb>`

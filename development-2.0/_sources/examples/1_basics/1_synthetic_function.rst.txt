
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/1_basics/1_synthetic_function.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_examples_1_basics_1_synthetic_function.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_1_basics_1_synthetic_function.py:


Synthetic Function
^^^^^^^^^^^^^^^^^^

An example of applying SMAC to optimize a synthetic function (2D Rosenbrock function).

We use the black-box facade because it is designed for black-box function optimization.
The black-box facade uses a :term:`Gaussian Process<GP>` as its surrogate model.
The facade works best on numerical hyperparameter configuration space and should not
be applied to problems with large evaluation budgets (up to 1000 evaluations).

.. GENERATED FROM PYTHON SOURCE LINES 12-66




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Default value: 16916.0
    [INFO][initial_design.py:91] Ignoring `configs` and `n_configs` since `n_configs_per_hyperparameter` is given.
    [INFO][base_smbo.py:143] Running initial design...
    [INFO][initial_design.py:174] Retrieving 20 configurations for the initial design.
    [INFO][intensification.py:244] No incumbent provided in the first run. Sampling a new challenger...
    [INFO][intensification.py:443] First run and no incumbent provided. Challenger is assumed to be the incumbent.
    [INFO][intensification.py:603] Updated estimated cost of incumbent on 1 runs: 1102.7878
    [INFO][intensification.py:603] Updated estimated cost of incumbent on 2 runs: 1102.7878
    [INFO][intensification.py:603] Updated estimated cost of incumbent on 3 runs: 1102.7878
    [INFO][abstract_intensifier.py:345] Challenger (5.4656) is better than incumbent (1102.7878) on 3 runs.
    [INFO][abstract_intensifier.py:365] Changes in incumbent:
    [INFO][abstract_intensifier.py:368] --- x0: -0.9968221839517355 -> 0.03135360777378082
    [INFO][abstract_intensifier.py:368] --- x1: 4.30847043171525 -> -0.21179260686039925
    [INFO][abstract_intensifier.py:345] Challenger (4.9344) is better than incumbent (5.4656) on 3 runs.
    [INFO][abstract_intensifier.py:365] Changes in incumbent:
    [INFO][abstract_intensifier.py:368] --- x0: 0.03135360777378082 -> 3.1521441831066923
    [INFO][abstract_intensifier.py:368] --- x1: -0.21179260686039925 -> 9.991027821936083
    [INFO][facade.py:330] Final Incumbent: {'x0': 3.1521441831066923, 'x1': 9.991027821936083}
    [INFO][facade.py:331] Estimated cost: 4.934388186264153
    Incumbent value: 4.93






|

.. code-block:: default


    from ConfigSpace import Configuration, ConfigurationSpace, Float

    from smac import BlackBoxFacade, Scenario

    __copyright__ = "Copyright 2021, AutoML.org Freiburg-Hannover"
    __license__ = "3-clause BSD"


    class Rosenbrock2D:
        @property
        def configspace(self) -> ConfigurationSpace:
            cs = ConfigurationSpace(seed=0)
            x0 = Float("x0", (-5, 10), default=-3)
            x1 = Float("x1", (-5, 10), default=-4)
            cs.add_hyperparameters([x0, x1])

            return cs

        def train(self, config: Configuration) -> float:
            """The 2-dimensional Rosenbrock function as a toy model.
            The Rosenbrock function is well know in the optimization community and
            often serves as a toy problem. It can be defined for arbitrary
            dimensions. The minimium is always at x_i = 1 with a function value of
            zero. All input parameters are continuous. The search domain for
            all x's is the interval [-5, 10].
            """
            x1 = config["x0"]
            x2 = config["x1"]

            cost = 100.0 * (x2 - x1**2.0) ** 2.0 + (1 - x1) ** 2.0
            return cost


    if __name__ == "__main__":
        model = Rosenbrock2D()

        # Scenario object
        scenario = Scenario(model.configspace, n_trials=100)

        # Example call of the target algorithm
        default_value = model.train(model.configspace.get_default_configuration())
        print(f"Default value: {round(default_value, 2)}")

        # Now we use SMAC to find the best hyperparameters
        smac = BlackBoxFacade(
            scenario,
            model.train,
            overwrite=True,
        )
        incumbent = smac.optimize()

        incumbent_value = model.train(incumbent)
        print(f"Incumbent value: {round(incumbent_value, 2)}")


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  1.388 seconds)


.. _sphx_glr_download_examples_1_basics_1_synthetic_function.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 1_synthetic_function.py <1_synthetic_function.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 1_synthetic_function.ipynb <1_synthetic_function.ipynb>`

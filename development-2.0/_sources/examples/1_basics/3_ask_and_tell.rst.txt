
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/1_basics/3_ask_and_tell.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_examples_1_basics_3_ask_and_tell.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_1_basics_3_ask_and_tell.py:


Ask-and-Tell Interface
^^^^^^^^^^^^^^^^^^

This examples show how to use the Ask-and-Tell interface.

.. GENERATED FROM PYTHON SOURCE LINES 7-88




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Default value: 16916.0
    [INFO][abstract_initial_design.py:81] Reducing the number of initial configurations from 20 to 10 (max_ratio == 0.1).
    [INFO][abstract_intensifier.py:336] Challenger (nan) is better than incumbent (nan) on 1 trials.
    [INFO][abstract_intensifier.py:336] Challenger (145.2763) is better than incumbent (1280.371) on 1 trials.
    [INFO][abstract_intensifier.py:360] Changes in incumbent:
    [INFO][abstract_intensifier.py:363] --- x0: 3.232202558909872 -> 3.1732477449534535
    [INFO][abstract_intensifier.py:363] --- x1: 6.87587557123997 -> 8.883949574389916
    [INFO][abstract_initial_design.py:132] Using 10 initial design and 0 additional configurations.
    [INFO][base_smbo.py:434] Starting optimization with incumbent {'x0': 3.1732477449534535, 'x1': 8.883949574389916}.
    [INFO][abstract_intensifier.py:336] Challenger (17.4566) is better than incumbent (145.2763) on 1 trials.
    [INFO][abstract_intensifier.py:360] Changes in incumbent:
    [INFO][abstract_intensifier.py:363] --- x1: 8.883949574389916 -> 9.712659380377252
    [INFO][abstract_intensifier.py:336] Challenger (6.4945) is better than incumbent (17.4566) on 1 trials.
    [INFO][abstract_intensifier.py:360] Changes in incumbent:
    [INFO][abstract_intensifier.py:363] --- x0: 3.1732477449534535 -> 3.1726141697734143
    [INFO][abstract_intensifier.py:363] --- x1: 9.712659380377252 -> 9.932280455690103
    [INFO][abstract_intensifier.py:336] Challenger (4.681) is better than incumbent (6.4945) on 1 trials.
    [INFO][abstract_intensifier.py:360] Changes in incumbent:
    [INFO][abstract_intensifier.py:363] --- x0: 3.1726141697734143 -> 3.1520883436957092
    [INFO][abstract_intensifier.py:363] --- x1: 9.932280455690103 -> 9.957920629061277
    [INFO][abstract_intensifier.py:336] Challenger (4.5449) is better than incumbent (4.681) on 1 trials.
    [INFO][abstract_intensifier.py:360] Changes in incumbent:
    [INFO][abstract_intensifier.py:363] --- x0: 3.1520883436957092 -> 3.025818356643253
    [INFO][abstract_intensifier.py:363] --- x1: 9.957920629061277 -> 9.089168816020091
    [INFO][abstract_intensifier.py:336] Challenger (4.4407) is better than incumbent (4.5449) on 1 trials.
    [INFO][abstract_intensifier.py:360] Changes in incumbent:
    [INFO][abstract_intensifier.py:363] --- x0: 3.025818356643253 -> 2.996593674320672
    [INFO][abstract_intensifier.py:363] --- x1: 9.089168816020091 -> 9.046979897585258
    [INFO][abstract_intensifier.py:336] Challenger (3.9789) is better than incumbent (4.4407) on 1 trials.
    [INFO][abstract_intensifier.py:360] Changes in incumbent:
    [INFO][abstract_intensifier.py:363] --- x0: 2.996593674320672 -> 2.990486118970205
    [INFO][abstract_intensifier.py:363] --- x1: 9.046979897585258 -> 8.930037581689849
    [INFO][abstract_facade.py:280] Final Incumbent: {'x0': 2.990486118970205, 'x1': 8.930037581689849}
    [INFO][abstract_facade.py:281] Estimated cost: 3.978856161714655
    Incumbent value: 3.98






|

.. code-block:: default


    from ConfigSpace import Configuration, ConfigurationSpace, Float

    from smac import HyperparameterFacade, Scenario
    from smac.runhistory.dataclasses import TrialInfo, TrialValue

    __copyright__ = "Copyright 2021, AutoML.org Freiburg-Hannover"
    __license__ = "3-clause BSD"


    class Rosenbrock2D:
        @property
        def configspace(self) -> ConfigurationSpace:
            cs = ConfigurationSpace(seed=0)
            x0 = Float("x0", (-5, 10), default=-3)
            x1 = Float("x1", (-5, 10), default=-4)
            cs.add_hyperparameters([x0, x1])

            return cs

        def train(self, config: Configuration, seed: int = 0) -> float:
            """The 2-dimensional Rosenbrock function as a toy model.
            The Rosenbrock function is well know in the optimization community and
            often serves as a toy problem. It can be defined for arbitrary
            dimensions. The minimium is always at x_i = 1 with a function value of
            zero. All input parameters are continuous. The search domain for
            all x's is the interval [-5, 10].
            """
            x1 = config["x0"]
            x2 = config["x1"]

            cost = 100.0 * (x2 - x1**2.0) ** 2.0 + (1 - x1) ** 2.0
            return cost


    if __name__ == "__main__":
        model = Rosenbrock2D()

        # Scenario object
        scenario = Scenario(
            model.configspace,
            deterministic=False,
            n_trials=100,
        )

        # Example call of the target function
        default_value = model.train(model.configspace.get_default_configuration())
        print(f"Default value: {round(default_value, 2)}")

        intensifier = HyperparameterFacade.get_intensifier(
            scenario,
            max_config_calls=1,  # We basically use one seed only
        )

        # Now we use SMAC to find the best hyperparameters
        smac = HyperparameterFacade(
            scenario,
            model.train,
            intensifier=intensifier,
            overwrite=True,
        )

        # We can provide SMAC with custom configurations first
        for config in model.configspace.sample_configuration(10):
            for seed in smac.get_target_function_seeds():
                cost = model.train(config, seed=seed)

                trial_info = TrialInfo(config, seed=seed)
                trial_value = TrialValue(cost=cost, time=0.5)

                smac.tell(trial_info, trial_value)

        # NOTE: Using only the tell method does not increase the number of submitted trials.
        # Hence, the optimize method will process additional 100 trials.
        smac.optimize()

        # Now we retrieve the best configuration using our runhistory
        incumbent = smac.runhistory.get_incumbent()

        incumbent_value = model.train(incumbent)
        print(f"Incumbent value: {round(incumbent_value, 2)}")


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.876 seconds)


.. _sphx_glr_download_examples_1_basics_3_ask_and_tell.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 3_ask_and_tell.py <3_ask_and_tell.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 3_ask_and_tell.ipynb <3_ask_and_tell.ipynb>`

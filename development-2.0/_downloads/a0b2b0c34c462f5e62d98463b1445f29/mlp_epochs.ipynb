{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multi-Layer Perceptron Using Multiple Epochs\n\nExample for optimizing a Multi-Layer Perceptron (MLP) using multiple budgets.\nSince we want to take advantage of Multi-Fidelity, the multi-fidelity facade is a good choice. By default,\n``MultiFidelityFacade`` internally runs with [hyperband](https://arxiv.org/abs/1603.06560), which is a combination of an\naggressive racing mechanism and successive halving.\n\nMLP is a deep neural network, and therefore, we choose epochs as fidelity type. The digits dataset\nis chosen to optimize the average accuracy on 5-fold cross validation.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This example uses the ``MultiFidelityFacade`` facade, which is the closest implementation to\n    [BOHB](https://github.com/automl/HpBandSter).</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\n\nlogging.basicConfig(level=logging.INFO)\n\nimport warnings\n\nimport numpy as np\nfrom ConfigSpace import (\n    Categorical,\n    ConfigurationSpace,\n    EqualsCondition,\n    Float,\n    InCondition,\n    Integer,\n)\nfrom sklearn.datasets import load_digits\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\n\nfrom smac import MultiFidelityFacade, Scenario\nfrom smac.configspace import ConfigurationSpace\n\n__copyright__ = \"Copyright 2021, AutoML.org Freiburg-Hannover\"\n__license__ = \"3-clause BSD\"\n\n\ndigits = load_digits()\n\n\nclass MLP:\n    @property\n    def configspace(self):\n        # Build Configuration Space which defines all parameters and their ranges.\n        # To illustrate different parameter types, we use continuous, integer and categorical parameters.\n        cs = ConfigurationSpace()\n\n        n_layer = Integer(\"n_layer\", (1, 5), default=1)\n        n_neurons = Integer(\"n_neurons\", (8, 256), log=True, default=10)\n        activation = Categorical(\"activation\", [\"logistic\", \"tanh\", \"relu\"], default=\"tanh\")\n        solver = Categorical(\"solver\", [\"lbfgs\", \"sgd\", \"adam\"], default=\"adam\")\n        batch_size = Integer(\"batch_size\", (30, 300), default=200)\n        learning_rate = Categorical(\"learning_rate\", [\"constant\", \"invscaling\", \"adaptive\"], default=\"constant\")\n        learning_rate_init = Float(\"learning_rate_init\", (0.0001, 1.0), default=0.001, log=True)\n\n        # Add all hyperparameters at once:\n        cs.add_hyperparameters(\n            [\n                n_layer,\n                n_neurons,\n                activation,\n                solver,\n                batch_size,\n                learning_rate,\n                learning_rate_init,\n            ]\n        )\n\n        # Adding conditions to restrict the hyperparameter space...\n        # ... since learning rate is used when solver is 'sgd'.\n        use_lr = EqualsCondition(child=learning_rate, parent=solver, value=\"sgd\")\n        # ... since learning rate initialization will only be accounted for when using 'sgd' or 'adam'.\n        use_lr_init = InCondition(child=learning_rate_init, parent=solver, values=[\"sgd\", \"adam\"])\n        # ... since batch size will not be considered when optimizer is 'lbfgs'.\n        use_batch_size = InCondition(child=batch_size, parent=solver, values=[\"sgd\", \"adam\"])\n\n        # We can also add multiple conditions on hyperparameters at once:\n        cs.add_conditions([use_lr, use_batch_size, use_lr_init])\n\n        return cs\n\n    def train(self, config, seed=0, budget=25):\n        # For deactivated parameters, the configuration stores None-values.\n        # This is not accepted by the MLP, so we replace them with placeholder values.\n        lr = config[\"learning_rate\"] if config[\"learning_rate\"] else \"constant\"\n        lr_init = config[\"learning_rate_init\"] if config[\"learning_rate_init\"] else 0.001\n        batch_size = config[\"batch_size\"] if config[\"batch_size\"] else 200\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n\n            classifier = MLPClassifier(\n                hidden_layer_sizes=[config[\"n_neurons\"]] * config[\"n_layer\"],\n                solver=config[\"solver\"],\n                batch_size=batch_size,\n                activation=config[\"activation\"],\n                learning_rate=lr,\n                learning_rate_init=lr_init,\n                max_iter=int(np.ceil(budget)),\n                random_state=seed,\n            )\n\n            # Returns the cross validation accuracy\n            cv = StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)  # to make CV splits consistent\n            score = cross_val_score(classifier, digits.data, digits.target, cv=cv, error_score=\"raise\")\n\n        return 1 - np.mean(score)\n\n\nif __name__ == \"__main__\":\n    mlp = MLP()\n    default_config = mlp.configspace.get_default_configuration()\n\n    # Example call of the target algorithm\n    default_value = mlp.train(default_config)\n    print(f\"Default value: {round(default_value, 2)}\")\n\n    # Define our environment variables\n    scenario = Scenario(\n        mlp.configspace,\n        walltime_limit=40,  # After 40 seconds, we stop the optimization\n        n_trials=100,  # Evaluate max 100 different trials\n        min_budget=5,  # Train the MLP for at least 5 epochs\n        max_budget=25,  # Train the MLP for at most 25 epochs\n    )\n\n    # We want to run five random configurations before starting the optimization.\n    initial_design = MultiFidelityFacade.get_initial_design(scenario, n_configs=5)\n\n    # Create our SMAC object and pass the scenario and the train method\n    smac = MultiFidelityFacade(\n        scenario,\n        mlp.train,\n        initial_design=initial_design,\n        overwrite=True,\n    )\n    incumbent = smac.optimize()\n\n    incumbent_value = mlp.train(incumbent)\n    print(f\"Incumbent value: {round(incumbent_value, 2)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}